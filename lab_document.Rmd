---
title: "DS 3001 Text Mining Lab"
author: "Group 10"
date: "3/24/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, include = FALSE, echo = FALSE, results = FALSE}
library(tidyverse)
library(tidytext)
library(textdata)
# install.packages("tidyselect")
library(tidyselect)
library(stringr)
library(DT)
# install.packages("topicmodels")
library(tm)
library(plotly)
library(topicmodels)
library(ggplot2)
library(ggwordcloud)
# save.image("innovedu.RData")
```


# Problem Statement
The purpose of this lab was to track the level of support nationally and regionally for the field of Data Science. The goal was to get a general idea of patterns associated with articles being written on the broad topic of data science. To do this, our team explored periodicals from around the country to track relative positive or negative sentiments and word frequencies. We used the library search engine NexusUni to find newspapers from different regions of the U.S. We also divided up the country into 3 main regions for analysis - the east coast, west coast, and midwest. From each region, two main newspapers were selected with no more than 100 articles each on the topic of data science.


# Newspapers Selected{.tabset}

## Eastern Newspapers
Pittsburgh Gazette - Pennsylvania

New York Times - New York 

## Midwestern Newspapers

The Telegraph Herald - Iowa

The Daily Oklahoman - Oklahoma

## Western Newspapers
The LA Times - California

Spokesman Review - Washington

# Import the data
```{r}
midwest_file_1 = "mid_west_file_1_1.rtf"
midwest_file_2 = "mid_west_file_2_1.rtf"
east_file_1 = "NYFiles.RTF"
east_file_2 = "PhillyFiles.RTF"
west_file_1 = "California_LATimes_docs.RTF"
west_file_2 = "Washington_docs.RTF"

#todo add your file names here
```

## Process and clean data for text analysis
```{r}
# function to clean and process the rtf files
process_rtf = function(file_name){
  
  rtf_result = read_lines(file_name)
  
  rtf_tibble = tibble(rtf_result) %>%
    mutate(rtf_result = as.character(rtf_result))
  
  # remove all the info contained in {} and some giberish
  rtf_tibble <- rtf_tibble %>%
    unnest_tokens(word, rtf_result) 
  
  # remove unimportant words
  rtf_tibble = rtf_tibble %>% 
    anti_join(stop_words) 
  
  # find the count of each word
  rtf_tibble = rtf_tibble %>%
    count(word, sort = TRUE)
  # remove rows with numbers on it
  rtf_tibble <- rtf_tibble %>% 
    filter(!grepl("[[:digit:]]", word)) 
  # remove na
  rtf_tibble <- rtf_tibble %>%
    filter(!str_detect(word, "na"))
  
  return(rtf_tibble)

}
```

```{r}
midwest_1 = process_rtf(midwest_file_1)
midwest_2 = process_rtf(midwest_file_2)
east_1 = process_rtf(east_file_1)
east_2 = process_rtf(east_file_2)
west_1 = process_rtf(west_file_1)
west_2 = process_rtf(west_file_2)

#todo add your file names and create your usable datasets
```

## Combine newspapers into regions
```{r}
combined_newspapers_into_regions = function(paper1, paper2){
  temp = rbind(paper1, paper2) %>%
  group_by(word) %>%
  summarize(n=sum(n)) %>%
  arrange(-n)
  
  return(temp)
}
```

```{r}
midwest = combined_newspapers_into_regions(midwest_1, midwest_2)
east = combined_newspapers_into_regions(east_1, east_2)
west = combined_newspapers_into_regions(west_1, west_2)
#todo add your regions here
```

# Sentiment analysis
```{r}
# load sentiment data
get_sentiments('afinn')
get_sentiments('nrc')
get_sentiments('bing')
```

## Create sentiment analysis for each region
```{r}
sentiment = function(raw){
  
  afinn = raw %>%
    inner_join(get_sentiments("afinn"))
  
  nrc = raw %>%
    inner_join(get_sentiments("nrc"))
  
  bing = raw %>%
    inner_join(get_sentiments("bing"))
  
  return(list("afinn" = afinn, "nrc" = nrc, "bing" = bing))
}
```

```{r}
midwest_sent = sentiment(midwest)
midwest_afinn = midwest_sent$afinn
midwest_nrc = midwest_sent$nrc
midwest_bing = midwest_sent$bing

east_sent = sentiment(east)
east_afinn = east_sent$afinn
east_nrc = east_sent$nrc
east_bing = east_sent$bing

west_sent = sentiment(west)
west_afinn = west_sent$afinn
west_nrc = west_sent$nrc
west_bing = west_sent$bing

# todo add your region and sentiment analysis here
```


# Plots
```{r}

# a function that takes in afinn data and readies it for the creation of a histogram (fancy bar plot)
afinn_to_histogram_data = function(afinn_data){
  temp = afinn_data %>%
    group_by(value)%>%
    summarize(n=n())
  
  return(temp)
}

# create the afinn plot we desire
afinn_plot = function(afinn_data){
  plt = afinn_data %>%
    ggplot(
      aes(
        x=value,
        y=n,
        fill=n
      )
    )+
    geom_bar(
      stat='identity'
    )+
    geom_smooth(
      method='loess',
      se=F,
      formula='y~x'
    ) + 
    scale_x_continuous(
    breaks = seq(-5, 6, by = 1),
    limits = c(-5, 6)
  ) 
  
  return(plt)
}
```


## Histogram

### Combine regional data for histogram
```{r}

# prep the data for the histogram
midwest_afinn_hist_data = midwest_afinn %>%
  afinn_to_histogram_data() %>%
  mutate(region = 'midwest')

east_afinn_hist_data = east_afinn %>%
  afinn_to_histogram_data() %>%
  mutate(region = 'east')

# todo add west region here
west_afinn_hist_data = west_afinn %>%
  afinn_to_histogram_data() %>%
  mutate(region = 'west')

# todo add west region to the rbind statement
all_regions_afinn_hist_data = rbind(midwest_afinn_hist_data, east_afinn_hist_data, west_afinn_hist_data)
```

```{r}
all_regions_afinn_hist_data %>%
  afinn_plot() +
  facet_wrap(
    .~region
  ) 
```


## Cloud Plot
```{r}
afinn_cloud_plot = function(afinn_data){
  plt = afinn_data %>%
  ggplot(
    aes(
      label=word,
      size=n
    )
  )+
  geom_text_wordcloud(
  )+
  theme_minimal()
  
  return(plt)
}

```


### Combine data for cloud plot
```{r}
midwest_afinntop50 <- midwest_afinn %>% 
  mutate(region = 'midwest') 
midwest_afinntop50 <- head(arrange(midwest_afinntop50, desc(n)), 50) 

east_afinntop50 <- east_afinn %>% 
  mutate(region = 'east')
east_afinntop50 <-  head(arrange(east_afinntop50, desc(n)), 50) 

west_afinntop50 <- west_afinn %>% 
  mutate(region = 'west')
west_afinntop50 <- head(arrange(west_afinntop50, desc(value)), 50) 

all_regions_afinn = rbind(midwest_afinntop50, east_afinntop50, west_afinntop50)
```

```{r}
all_regions_afinn %>%
  afinn_cloud_plot(
  )+
  facet_wrap(
    .~region 
  )
```

# TF-IDF Analysis
```{r}

data_prep <- function(x,y,z){
  i <- as_tibble(t(x))
  ii <- unite(i,"text",y:z,remove = TRUE,sep = "")
}
# prepares right data format for TF-IDF
TFIDF_prep <- function(regiontext){
  data_raw <- as.tibble(read_lines(regiontext))
  data_bag <- data_prep(data_raw, 1, nrow(data_raw))
  
}
# list of newspapers used
newspapers <- c("NewYorkTimes", "PittsburghGazette", "TelegraphHerald", "DailyOklahoman", "LATimes", "SpokesmanReview")


east_1_bag <- TFIDF_prep(east_file_1)
east_2_bag <- TFIDF_prep(east_file_2)
midwest_1_bag <- TFIDF_prep(midwest_file_1)
midwest_2_bag <- TFIDF_prep(midwest_file_2)
west_1_bag <- TFIDF_prep(west_file_1)
west_2_bag <- TFIDF_prep(west_file_2)

tf_idf_text <- tibble(newspapers,text=t(tibble(east_1_bag, east_2_bag, midwest_1_bag, midwest_2_bag,west_1_bag, west_2_bag, .name_repair = "universal")))

# todo add West newspapers to list 

word_count <- tf_idf_text %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  count(newspapers, word, sort = TRUE)

word_count <- word_count %>% 
  filter(!grepl("[[:digit:]]", word))

word_count <- word_count%>%
  inner_join(get_sentiments("afinn"))

word_count <- word_count[,-c(4)]

total_words <- word_count %>% 
  group_by(newspapers) %>% 
  summarize(total = sum(n))
newswords <- left_join(word_count, total_words)

newswords <- newswords %>%
  bind_tf_idf(word, newspapers, n)


```
## TF-IDF Plot

```{r}
newswords_top10 <- head(arrange(newswords, desc(tf_idf)), 10)
newswords_top10
IDFplot <- newswords_top10 %>%
  ggplot(
    aes(
      x = word,
      y = tf_idf,
      fill = newspapers
      )
    ) +
  geom_bar(
      stat='identity'
    ) + 
  labs(
    x = "Word",
    y = "TF-IDF Value",
    title = "Top 10 Highest TF-IDF Words"
  )
  
  
IDFplot
```

# Analysis
<<<<<<< HEAD

Your main goal (and the goal of all practicing data scientists!) is to translate this information into action. What patterns do you see, why do you believe this to be the case? What additional information might you want? Be as specific as possible, but keep in mind this is an initial exploratory effort...more analysis might be needed...but the result can and should advise the next steps you present to the firm. 


Please submit a cleanly knitted HTML file describing in detail the steps you took along the way, the results of your analysis and most importantly the implications/next steps you would recommend.  You will report your final results and recommendations next week in class. This will be 5 minutes per group. 

=======
  The six newspapers we chose were the New York Times, the Pittsburgh Gazette, the Telegraph Herald, the Daily Oklahoma, . To do the sentiment analysis we used afinn, bing, and nrc. As seen in the histogram using afinn sentiment analysis, a trend that we noticed with the overall sentiment presented in the newspapers were that the words used by the newspapers were fairly negative. Commonly shared words between the East, the Midwest, and the West were innovation, care, united, and support. 
>>>>>>> 919f51745c276f84e348ef468151f2e0e2e7a1bd
# Conclusion















































